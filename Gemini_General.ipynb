{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini API: General Parameters\n",
        "This notebook covers:\n",
        "\n",
        "* Temperature\n",
        "* Max output length\n",
        "* Token counting"
      ],
      "metadata": {
        "id": "mk3uL_1mnllw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SetUp\n",
        "\n"
      ],
      "metadata": {
        "id": "P0hY_e42n3pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "glCPSFgPn-QN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "qIuMt7tPoPFK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "LiqtFW7LoURR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Temperature\n",
        "Steve is indecisive about how to spend his Friday night as a Freshman at Berkeley. He decides to ask Gemini using the 1.5 Flash variant:"
      ],
      "metadata": {
        "id": "no3NS8INotKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Cc_mWectowSk",
        "outputId": "0c148773-5aa9-4f70-d31e-22e0e1089201"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hack a fun project with friends, fueled by copious amounts of caffeine and questionable pizza.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steve is very smart and knows that Gemini is non-deterministic; the same prompt can result in different outputs! To demonstrate this, the following code passes the same prompt 5 different times:"
      ],
      "metadata": {
        "id": "MjZOyTu3pBOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "for i in range(5):\n",
        "  response = model.generate_content(prompt)\n",
        "  outputs.append(response.text)\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lW-Sm1XBpLzn",
        "outputId": "54a2eb79-c524-48c3-8d1b-69e52ec84c88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Hack a (legal and ethical) project with friends, fueled by copious amounts of caffeine and questionable pizza.\n",
            "\n",
            "2. Grab some friends, conquer a challenging coding problem on HackerRank while fueled by late-night pizza and competitive spirit.\n",
            "\n",
            "3. Grab some fellow CS majors, head to a nearby boba shop for a caffeine-fueled coding competition/hackathon fueled by late-night pizza.\n",
            "\n",
            "4. Hack on a fun side project with friends, fueled by copious amounts of caffeine and pizza.\n",
            "\n",
            "5. Grab some friends, conquer a challenging coding problem on HackerRank over boba and pizza, fueled by the thrill of competitive programming and the promise of weekend relaxation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini returns a different response each time. Hmm. Steve doesn't like the fact that his Friday night might be determined by random chance.\n",
        "\n",
        "Fortunately, Gemini has a temperature parameter that controls the randomness of the output. Temperature values can range from 0.0 to 2.0. We can check the temperature of our current model as follows:"
      ],
      "metadata": {
        "id": "nZYa7D-CpbTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == 'models/gemini-1.5-flash':\n",
        "        print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "UddzH-mYpeXS",
        "outputId": "62358d27-dd46-4468-d941-8f9731fa5341"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initialize a new model with a temperature of 0:"
      ],
      "metadata": {
        "id": "VuyF6L5Hpquw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "new_outputs = []\n",
        "low_temp_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0})\n",
        "for i in range(5):\n",
        "  response = low_temp_model.generate_content(prompt)\n",
        "  new_outputs.append(response.text)\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4REn65tuptXa",
        "outputId": "90cbf994-54fc-44de-d728-04bb7eac65fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Grab some friends, conquer a challenging coding problem on HackerRank over boba tea, and then debate the merits of various programming languages until the wee hours.\n",
            "\n",
            "2. Grab some friends, conquer a challenging coding problem on HackerRank over boba tea, then debate the merits of various programming languages until sunrise.\n",
            "\n",
            "3. Grab some friends, conquer a challenging coding problem on HackerRank over boba and pizza, fueled by the thrill of collaborative problem-solving and the promise of weekend relaxation.\n",
            "\n",
            "4. Grab some friends, conquer a challenging coding problem on HackerRank over boba and pizza, fueled by the thrill of collaborative problem-solving and the promise of weekend relaxation.\n",
            "\n",
            "5. Grab some friends, conquer a challenging coding problem on HackerRank over boba tea, then debate the merits of various programming languages until sunrise.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs are the same! Finally Steve can be sure how to spend his night. Conversely, setting temperature to the max value of 2.0 would have the opposite effect, yielding more unpredictable responses."
      ],
      "metadata": {
        "id": "JkX1zmRvp_GP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max Output Length\n",
        "Let's say Steve wants his outputs to be below a certain length. In large language models, text is generated in tokens. For Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words. He can set the max_output_tokens variable as follows:"
      ],
      "metadata": {
        "id": "j7rTPRv1qOZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "short_response_model = genai.GenerativeModel(model_name, generation_config={\"max_output_tokens\": 5})\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = short_response_model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ui58cmXPqREz",
        "outputId": "ed0922bc-0a02-4770-ed13-2f7b82ec952f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hack a collaborative coding project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this simply halts token generation at a fixed quantity and does not guarantee that the output is complete."
      ],
      "metadata": {
        "id": "DJO_Ia9xqVXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Count\n",
        "Let's say that Steve is being charged for every token that he inputs to Gemini.\n",
        "\n",
        "If Steve has billing enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important. As such, Steve might want to know the number of tokens in his prompt (input) before actually putting it into the model.\n",
        "\n",
        "Let's create a new instance of Gemini 1.5 Flash and set our prompt:"
      ],
      "metadata": {
        "id": "aq3Mk3N-qd3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "token_n_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.0})\n",
        "poem_prompt = \"Write me a poem about Berkeley's campus\""
      ],
      "metadata": {
        "id": "MbqcqjYAqiFw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = token_n_model.generate_content(poem_prompt)\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "82hm59_zqqna",
        "outputId": "cab2c640-427c-4514-aabd-72dbc83e3b2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Campanile's chime, a brazen call,\\nAcross the bay, its shadow falls.\\nOn Sather Tower, the sunbeams gleam,\\nA golden crown, a student's dream.\\n\\nThrough Strawberry Canyon, paths wind slow,\\nWhere redwood giants softly grow.\\nA whisper stirs in eucalyptus leaves,\\nAs knowledge breathes, and spirit weaves.\\n\\nThe scent of incense, faintly sweet,\\nOn Sproul Plaza, where students meet.\\nA tapestry of voices rise,\\nBeneath the ever-changing skies.\\n\\nFrom Bancroft's halls, to Wheeler's might,\\nA vibrant pulse, both day and night.\\nThe murmur soft of library lore,\\nAnd protests echoing evermore.\\n\\nThe Golden Bear, a watchful gaze,\\nThrough changing times, in sunlit haze.\\nA campus steeped in history's grace,\\nA vibrant heart, a timeless space.\\n\\nSo wander on, explore and find,\\nThe beauty held within this mind,\\nOf Berkeley's hills, a grand domain,\\nWhere learning blooms, again, again.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating a response, we can check how many tokens are in this prompt using the `.count_tokens` function of the model:"
      ],
      "metadata": {
        "id": "GRU7eRCAqxa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_count = token_n_model.count_tokens(poem_prompt)\n",
        "output_token_count = token_n_model.count_tokens(response.text)\n",
        "print(f'Tokens in prompt: {prompt_token_count} \\n Estimated tokens in output {output_token_count}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "BMmJDUOMq0Kn",
        "outputId": "55fdbe4a-f255-4333-c4ab-ede0c3bfb5d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in prompt: total_tokens: 9\n",
            " \n",
            " Estimated tokens in output total_tokens: 223\n",
            "\n"
          ]
        }
      ]
    }
  ]
}